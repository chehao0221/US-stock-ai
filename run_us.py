import yfinance as yf
import pandas as pd
import numpy as np
import requests
import os
import warnings
from xgboost import XGBRegressor
from datetime import datetime, timedelta

# =========================
# åŸºæœ¬è¨­å®š
# =========================
warnings.filterwarnings("ignore")
WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "").strip()
HISTORY_FILE = "us_sp500_history.csv"

def get_sp500_300_pool():
    """å¾ç¶­åŸºç™¾ç§‘æŠ“å– S&P 500 æ¸…å–®ä¸¦å–å‰ 300 æª”"""
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=15)
        df = pd.read_html(res.text)[0]
        # ç¾è‚¡ä»£ç¢¼è½‰æ› (ä¾‹å¦‚ BRK.B è½‰ BRK-B)
        symbols = [s.replace('.', '-') for s in df['Symbol'].tolist()]
        return symbols[:300]
    except Exception as e:
        print(f"ç²å– S&P 500 æ¸…å–®å¤±æ•—: {e}")
        # å‚™ç”¨æ ¸å¿ƒæ¬Šå€¼è‚¡
        return ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA", "AVGO"]

def compute_features(df):
    """è¨ˆç®—ç¾è‚¡ç‰¹å¾µæŒ‡æ¨™"""
    df = df.copy()
    # åƒ¹æ ¼è®Šå‹•èˆ‡å‹•èƒ½
    df["mom20"] = df["Close"].pct_change(20)
    
    # RSI
    delta = df["Close"].diff()
    gain = delta.clip(lower=0).rolling(14).mean()
    loss = (-delta.clip(upper=0)).rolling(14).mean()
    df["rsi"] = 100 - (100 / (1 + gain / (loss + 1e-9)))
    
    # ä¹–é›¢ç‡èˆ‡é‡èƒ½æ¯”
    df["ma20"] = df["Close"].rolling(20).mean()
    df["bias"] = (df["Close"] - df["ma20"]) / (df["ma20"] + 1e-9)
    df["vol_ratio"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)
    
    # æ”¯æ’å£“åŠ›ä½
    df["sup"] = df["Low"].rolling(60).min()
    df["res"] = df["High"].rolling(60).max()
    return df

def audit_and_save(results, top_keys):
    """å°å¸³èˆ‡å­˜æª”é‚è¼¯"""
    audit_msg = ""
    today = datetime.now().date()
    
    if os.path.exists(HISTORY_FILE):
        hist = pd.read_csv(HISTORY_FILE)
        hist['date'] = pd.to_datetime(hist['date']).dt.date
        
        # æª¢æŸ¥ 7 å¤©å‰çš„é æ¸¬
        deadline = today - timedelta(days=7)
        unsettled = hist[(hist['settled'] == False) & (hist['date'] <= deadline)]
        
        if not unsettled.empty:
            audit_msg = "\nğŸ¯ **US 5-Day Prediction Audit**\n"
            for idx, r in unsettled.iterrows():
                try:
                    p_df = yf.Ticker(r["symbol"]).history(period="1d")
                    if p_df.empty: continue
                    curr_p = p_df["Close"].iloc[-1]
                    act_ret = (curr_p - r["pred_p"]) / r["pred_p"]
                    hit = "âœ…" if np.sign(act_ret) == np.sign(r["pred_ret"]) else "âŒ"
                    audit_msg += f"`{r['symbol']}`: {r['pred_ret']:+.2%} â” {act_ret:+.2%} {hit}\n"
                    hist.at[idx, "settled"] = True
                except: continue
        hist.to_csv(HISTORY_FILE, index=False)
    else:
        hist = pd.DataFrame(columns=["date", "symbol", "pred_p", "pred_ret", "settled"])

    # å­˜å…¥ä»Šæ—¥é æ¸¬
    new_rows = [{"date": today, "symbol": s, "pred_p": results[s]["c"], "pred_ret": results[s]["p"], "settled": False} for s in top_keys]
    hist = pd.concat([hist, pd.DataFrame(new_rows)], ignore_index=True)
    hist.to_csv(HISTORY_FILE, index=False)
    return audit_msg

def run():
    print("ğŸš€ å•Ÿå‹•ç¾è‚¡ S&P 300 AI æƒæ...")
    watch_pool = get_sp500_300_pool()
    must_watch = ["AAPL", "NVDA", "TSLA", "MSFT", "GOOGL"]
    all_syms = list(set(watch_pool + must_watch))
    
    # æ‰¹é‡æŠ“å–è³‡æ–™ (ç¾è‚¡å»ºè­°æŠ“ 2 å¹´å³å¯æ»¿è¶³æŒ‡æ¨™è¨ˆç®—)
    data = yf.download(all_syms, period="2y", progress=False, group_by="ticker")
    
    results = {}
    feats = ["mom20", "rsi", "bias", "vol_ratio"]
    
    for s in all_syms:
        try:
            df = data[s].dropna()
            if len(df) < 80: continue
            
            df = compute_features(df)
            df["target"] = df["Close"].shift(-5) / df["Close"] - 1 # é ä¼° 5 æ—¥å¾Œ
            
            train = df.dropna()
            model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.05, random_state=42)
            model.fit(train[feats], train["target"])
            
            # ç²å–æœ€æ–°ä¸€ç­†é€²è¡Œé æ¸¬
            pred = float(np.clip(model.predict(df[feats].iloc[-1:])[0], -0.2, 0.2))
            results[s] = {
                "p": pred, 
                "c": df["Close"].iloc[-1],
                "s": df["sup"].iloc[-1],
                "r": df["res"].iloc[-1]
            }
        except: continue

    # æ’åºå‰ 5 å (æ’é™¤å¿…çœ‹æ¨™çš„ï¼Œå°‹æ‰¾æ½›åŠ›è‚¡)
    top_5 = sorted([s for s in results if s not in must_watch], key=lambda x: results[x]['p'], reverse=True)[:5]
    audit_report = audit_and_save(results, top_5)
    
    # Discord å ±å‘Šæ’ç‰ˆ
    report_date = datetime.now().strftime("%Y-%m-%d %H:%M")
    msg = f"ğŸ‡ºğŸ‡¸ **ç¾è‚¡ AI é ä¼°å ±å‘Š (S&P 300) - {report_date}**\n"
    msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
    msg += "ğŸ† **æœªä¾† 5 æ—¥æ¼²å¹… Top 5 æ½›åŠ›è‚¡**\n"
    
    ranks = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ“ˆ", "ğŸ“ˆ"]
    for idx, s in enumerate(top_5):
        i = results[s]
        msg += f"{ranks[idx]} **{s}**: `é ä¼° {i['p']:+.2%}`\n"
        msg += f"   â”” ç¾åƒ¹: `${i['c']:.2f}` (æ”¯æ’: {i['s']:.2f} / å£“åŠ›: {i['r']:.2f})\n"

    msg += "\nğŸ’¡ **æ ¸å¿ƒæ¬Šå€¼è‚¡è§€æ¸¬**\n"
    for s in must_watch:
        if s in results:
            i = results[s]
            msg += f"â­ **{s}**: `${i['c']:.2f}` | `é ä¼° {i['p']:+.2%}`\n"

    msg += audit_report + "\n*Risk Warning: Predictions are for educational purposes.*"
    
    # ç™¼é€é€šçŸ¥
    if WEBHOOK_URL:
        requests.post(WEBHOOK_URL, json={"content": msg})
    else:
        print("\n--- Discord Preview ---\n", msg)

if __name__ == "__main__":
    run()
