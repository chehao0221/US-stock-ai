import yfinance as yf
import pandas as pd
import numpy as np
import requests
import os
from xgboost import XGBRegressor
from datetime import datetime, timedelta
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# é…ç½®ç’°å¢ƒè®Šæ•¸
WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "").strip()
HISTORY_FILE = "us_history.csv"

def get_us_300_pool():
    """ç²å–æ¨™æ™®500å‰300æª”è‚¡ç¥¨æ± """
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        # æ¨¡æ“¬ç€è¦½å™¨æ¨™é ­ï¼Œé˜²æ­¢è¢«ç¶­åŸºç™¾ç§‘æ‹’çµ•
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        df = pd.read_html(response.text)[0]
        symbols = [s.replace('.', '-') for s in df['Symbol'].tolist()[:300]]
        print(f"æˆåŠŸç²å–è‚¡ç¥¨æ± ï¼Œå…± {len(symbols)} æª”è‚¡ç¥¨")
        return symbols
    except Exception as e:
        print(f"ç²å–æ¸…å–®å¤±æ•— ({e})ï¼Œä½¿ç”¨å‚™ç”¨æ¸…å–®")
        return ["AAPL", "NVDA", "TSLA", "MSFT", "GOOGL", "AMZN", "META", "AVGO", "COST", "NFLX"]

def compute_features(df):
    """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ"""
    df = df.copy()
    if len(df) < 30:
        return None
    
    # åŸºç¤æŒ‡æ¨™
    df["mom20"] = df["Close"].pct_change(20)
    df["rsi"] = 100 - (100 / (1 + df["Close"].diff().clip(lower=0).rolling(14).mean() / ((-df["Close"].diff().clip(upper=0)).rolling(14).mean() + 1e-9)))
    df["ma20"] = df["Close"].rolling(20).mean()
    df["bias"] = (df["Close"] - df["ma20"]) / (df["ma20"] + 1e-9)
    
    # æ”¯æ’å£“åŠ›ä½ (ç°¡æ˜“ç‰ˆ)
    df["sup"] = df["Low"].rolling(20).min()
    df["res"] = df["High"].rolling(20).max()
    
    # é æ¸¬ç›®æ¨™ï¼šæœªä¾† 5 å¤©çš„å ±é…¬ç‡
    df["target"] = df["Close"].shift(-5).pct_change(5)
    return df

def audit_and_save(results, top_5):
    """ä¿å­˜é æ¸¬çµæœåˆ° CSV"""
    new_records = []
    today_str = datetime.now().strftime("%Y-%m-%d")
    for s in top_5:
        if s in results:
            new_records.append({
                "date": today_str,
                "symbol": s,
                "pred_p": results[s]['p'],
                "pred_ret": results[s]['p'], # é æ¸¬å ±é…¬
                "settled": 0
            })
    
    if new_records:
        new_df = pd.DataFrame(new_records)
        if os.path.exists(HISTORY_FILE):
            old_df = pd.read_csv(HISTORY_FILE)
            pd.concat([old_df, new_df]).to_csv(HISTORY_FILE, index=False)
        else:
            new_df.to_csv(HISTORY_FILE, index=False)
    return True

def main():
    pool = get_us_300_pool()
    must_watch = ["AAPL", "NVDA", "MSFT", "TSLA"]
    results = {}
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=150)
    
    print("é–‹å§‹ä¸‹è¼‰è³‡æ–™èˆ‡æ¨¡å‹é æ¸¬...")
    for s in pool + must_watch:
        try:
            # ä¸‹è¼‰è³‡æ–™ï¼ŒåŠ å…¥ repair=True å¢åŠ æˆåŠŸç‡
            df = yf.download(s, start=start_date, end=end_date, progress=False, repair=True)
            
            if df is None or len(df) < 50:
                continue
                
            df = compute_features(df)
            if df is None: continue
            
            feats = ["mom20", "rsi", "bias"]
            train = df.dropna()
            
            if len(train) < 10: continue
            
            # XGBoost æ¨¡å‹
            model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.07)
            model.fit(train[feats], train["target"])
            
            # é€²è¡Œæœ€å¾Œä¸€å¤©çš„é æ¸¬
            last_row = df[feats].iloc[-1:]
            pred = model.predict(last_row)[0]
            
            results[s] = {
                "p": float(pred), 
                "c": float(df["Close"].iloc[-1]), 
                "s": float(df["sup"].iloc[-1]), 
                "r": float(df["res"].iloc[-1])
            }
        except Exception as e:
            print(f"è™•ç† {s} æ™‚å‡ºéŒ¯: {e}")
            continue

    # æ’åºï¼šæ’é™¤æ¬Šå€¼è‚¡å¾Œçš„ Top 5
    filtered_list = [s for s in results if s not in must_watch]
    top_5 = sorted(filtered_list, key=lambda x: results[x]['p'], reverse=True)[:5]
    
    # å­˜æª”
    audit_and_save(results, top_5)
    
    # å»ºç«‹ Discord è¨Šæ¯
    today = datetime.now().strftime("%Y-%m-%d %H:%M EST")
    msg = f"ğŸ‡ºğŸ‡¸ **ç¾è‚¡ AI é ä¼°å ±å‘Š ({today})**\n"
    msg += "----------------------------------\n"
    
    if not top_5:
        msg += "âš ï¸ ä»Šæ—¥ç„¡ç¬¦åˆæ¢ä»¶ä¹‹æ¨è–¦è‚¡ç¥¨ã€‚\n"
    else:
        msg += "ğŸ† **300 è‚¡ç¥¨å‰ 5 çš„æœªä¾†é ä¼°**\n"
        ranks = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ“ˆ", "ğŸ“ˆ"]
        for idx, s in enumerate(top_5):
            i = results[s]
            msg += f"{ranks[idx]} **{s}**: `é ä¼° {i['p']:+.2%}`\n"
            msg += f"   â”” ç¾åƒ¹: `${i['c']:.2f}` (æ”¯æ’: {i['s']:.1f} / å£“åŠ›: {i['r']:.1f})\n"

    msg += "\nğŸ’¡ **æ¬Šå€¼è‚¡è§€å¯Ÿ**\n"
    for s in must_watch:
        if s in results:
            i = results[s]
            msg += f"â€¢ **{s}**: `{i['p']:+.2%}` (ç¾åƒ¹: {i['c']:.2f})\n"

    # ç™¼é€ Discord Webhook
    if WEBHOOK_URL:
        requests.post(WEBHOOK_URL, json={"content": msg})
    else:
        print("æœªè¨­å®š Webhook URLï¼Œåƒ…è¼¸å‡ºçµæœï¼š")
        print(msg)

if __name__ == "__main__":
    main()
